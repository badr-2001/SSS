{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2e4e98775a164a69ba414820f3634587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_311cb2fb5a8140179af02acb1d62dffe",
              "IPY_MODEL_7493e997953f4effa1be93bc298df637",
              "IPY_MODEL_283a9450fb68466f8d2bd772057a679b"
            ],
            "layout": "IPY_MODEL_042b1674c6a0419e9180fd6589290099"
          }
        },
        "311cb2fb5a8140179af02acb1d62dffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecb1103738f54179a27df7ffd85c0337",
            "placeholder": "​",
            "style": "IPY_MODEL_a4ca28bfd9e84765a4a5e05ebfbd25e8",
            "value": "100%"
          }
        },
        "7493e997953f4effa1be93bc298df637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2dd924ad7494871ae4230f0e883bcb0",
            "max": 141,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15a178e0dfa8479c97496a03e650fbb5",
            "value": 141
          }
        },
        "283a9450fb68466f8d2bd772057a679b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5c9b78d1ccf4ab1ad584c95081d83ad",
            "placeholder": "​",
            "style": "IPY_MODEL_1f296fe3daf243a7b690d64ac44150bf",
            "value": " 141/141 [00:10&lt;00:00, 16.91it/s]"
          }
        },
        "042b1674c6a0419e9180fd6589290099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecb1103738f54179a27df7ffd85c0337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4ca28bfd9e84765a4a5e05ebfbd25e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2dd924ad7494871ae4230f0e883bcb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15a178e0dfa8479c97496a03e650fbb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5c9b78d1ccf4ab1ad584c95081d83ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f296fe3daf243a7b690d64ac44150bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Installe la bibliothèque Ultralytics\n",
        "!pip install ultralytics\n",
        "\n",
        "# Importe la fonction 'clear_output' du module 'display' pour effacer la sortie précédente\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "# Vérifie les dépendances Ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "\n",
        "# Importe le module 'os' pour travailler avec le système de fichiers\n",
        "import os\n",
        "\n",
        "# Obtient le répertoire de travail actuel\n",
        "HOME = os.getcwd()\n",
        "\n",
        "# Se déplace vers le répertoire de travail actuel\n",
        "%cd {HOME}\n",
        "\n",
        "# Clone le référentiel GitHub ByteTrack dans le répertoire actuel\n",
        "!git clone https://github.com/ifzhang/ByteTrack.git\n",
        "\n",
        "# Se déplace vers le répertoire ByteTrack\n",
        "%cd {HOME}/ByteTrack\n",
        "\n",
        "# Correction liée à un problème dans requirements.txt\n",
        "!sed -i 's/onnx==1.8.1/onnx==1.9.0/g' requirements.txt\n",
        "\n",
        "# Installe les dépendances à partir de requirements.txt\n",
        "!pip3 install -q -r requirements.txt\n",
        "\n",
        "# Installe le package ByteTrack en mode développement\n",
        "!python3 setup.py -q develop\n",
        "\n",
        "# Installe les packages cython_bbox et onemetric\n",
        "!pip install -q cython_bbox\n",
        "!pip install -q onemetric\n",
        "\n",
        "# Correction liée à des problèmes dans le notebook\n",
        "!pip install -q loguru lap thop\n",
        "\n",
        "# Efface la sortie précédente\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "# Ajoute le chemin vers le répertoire ByteTrack au chemin d'importation\n",
        "import sys\n",
        "sys.path.append(f\"{HOME}/ByteTrack\")\n",
        "\n",
        "# Importe le module yolox et affiche sa version\n",
        "import yolox\n",
        "print(\"yolox.__version__:\", yolox.__version__)\n",
        "\n",
        "# Installe la version spécifique du package supervision\n",
        "!pip install supervision==0.1.0\n",
        "display.clear_output()\n",
        "\n",
        "# Importe le module supervision et affiche sa version\n",
        "import supervision\n",
        "print(\"supervision.__version__:\", supervision.__version__)\n",
        "\n",
        "# Importe les modules nécessaires\n",
        "from supervision.tools.detections import Detections, BoxAnnotator\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from moviepy.editor import VideoFileClip\n",
        "import moviepy.video.fx.all as vfx\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import csv\n",
        "import math\n",
        "from ultralytics import YOLO\n",
        "import time\n",
        "from keras.models import load_model\n",
        "from supervision.draw.color import ColorPalette\n",
        "from supervision.geometry.dataclasses import Point\n",
        "from supervision.video.dataclasses import VideoInfo\n",
        "from supervision.video.source import get_video_frames_generator\n",
        "from supervision.video.sink import VideoSink\n",
        "from supervision.notebook.utils import show_frame_in_notebook\n",
        "from supervision.tools.detections import Detections, BoxAnnotator\n",
        "from typing import List\n",
        "from yolox.tracker.byte_tracker import BYTETracker, STrack\n",
        "from onemetric.cv.utils.iou import box_iou_batch\n",
        "from dataclasses import dataclass\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MLayikUjLt5",
        "outputId": "5b78a53e-81ef-41b3-f000-3a562c137868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "supervision.__version__: 0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Décorateur @dataclass pour définir une classe de configuration BYTETrackerArgs\n",
        "@dataclass(frozen=True)\n",
        "class BYTETrackerArgs:\n",
        "    # Seuil pour décider si une piste est activée ou non\n",
        "    track_thresh: float = 0.4\n",
        "\n",
        "    # Taille du tampon de piste (nombre maximal de trames de suivi)\n",
        "    track_buffer: int = 300\n",
        "\n",
        "    # Seuil de correspondance pour associer les détections aux pistes\n",
        "    match_thresh: float = 0.8\n",
        "\n",
        "    # Seuil de rapport d'aspect pour filtrer les boîtes englobantes basées sur leur forme\n",
        "    aspect_ratio_thresh: float = 3.0\n",
        "\n",
        "    # Surface minimale d'une boîte englobante pour être prise en compte\n",
        "    min_box_area: float = 1.0\n",
        "\n",
        "    # Indicateur pour activer/désactiver des paramètres spécifiques à mot20\n",
        "    mot20: bool = False\n"
      ],
      "metadata": {
        "id": "QUEyXP0Wgn5h"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cette fonction convertit les détections en un format pouvant être utilisé par la fonction match_detections_with_tracks\n",
        "def detections2boxes(detections: Detections) -> np.ndarray:\n",
        "    # Concatène les coordonnées xyxy des détections avec les confiances correspondantes\n",
        "    return np.hstack((\n",
        "        detections.xyxy,\n",
        "        detections.confidence[:, np.newaxis]\n",
        "    ))\n",
        "\n",
        "\n",
        "# Cette fonction convertit une liste de STrack en un format pouvant être utilisé par la fonction match_detections_with_tracks\n",
        "def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n",
        "    # Crée un tableau numpy contenant les boîtes (tlbr) des pistes\n",
        "    return np.array([\n",
        "        track.tlbr\n",
        "        for track\n",
        "        in tracks\n",
        "    ], dtype=float)\n",
        "\n",
        "\n",
        "# Cette fonction associe nos boîtes englobantes avec les prédictions\n",
        "def match_detections_with_tracks(\n",
        "    detections: Detections,\n",
        "    tracks: List[STrack]\n",
        ") -> Detections:\n",
        "    # Vérifie si les coordonnées xyxy des détections sont vides ou s'il n'y a pas de pistes\n",
        "    if not np.any(detections.xyxy) or len(tracks) == 0:\n",
        "        return np.empty((0,))\n",
        "\n",
        "    # Convertit les boîtes des pistes en un tableau numpy\n",
        "    tracks_boxes = tracks2boxes(tracks=tracks)\n",
        "\n",
        "    # Calcule l'indice de recouvrement IoU (Intersection over Union) entre les boîtes des pistes et les détections\n",
        "    iou = box_iou_batch(tracks_boxes, detections.xyxy)\n",
        "\n",
        "    # Pour chaque détection, trouve l'indice de la piste avec lequel elle a le plus grand IoU\n",
        "    track2detection = np.argmax(iou, axis=1)\n",
        "\n",
        "    # Initialise une liste vide pour stocker les ID des pistes associées aux détections\n",
        "    tracker_ids = [None] * len(detections)\n",
        "\n",
        "    # Associe les ID des pistes aux détections en fonction des indices d'association\n",
        "    for tracker_index, detection_index in enumerate(track2detection):\n",
        "        if iou[tracker_index, detection_index] != 0:\n",
        "            tracker_ids[detection_index] = tracks[tracker_index].track_id\n",
        "\n",
        "    # Retourne la liste des ID de pistes associées aux détections\n",
        "    return tracker_ids\n"
      ],
      "metadata": {
        "id": "qBS92xmzgq_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chemin d'accès à la vidéo source\n",
        "SOURCE_VIDEO_PATH = f\"/content/nvcxWZBO.mp4\"\n",
        "# Chemin d'accès à la vidéo cible (résultante)\n",
        "TARGET_VIDEO_PATH = f\"/content/result.mp4\"\n",
        "\n",
        "# Configuration\n",
        "MODEL = \"yolov8x.pt\"\n",
        "model = YOLO(MODEL)  # Crée une instance du modèle YOLO\n",
        "CLASS_NAMES_DICT = model.model.names  # Dictionnaire qui mappe les IDs de classe aux noms de classe\n",
        "CLASS_ID = [0]  # Liste d'IDs de classe d'intérêt (par exemple, ID de la classe \"voiture\")\n",
        "byte_tracker = BYTETracker(BYTETrackerArgs())  # Crée une instance de BYTETracker avec les arguments spécifiés\n",
        "video_info = VideoInfo.from_video_path(SOURCE_VIDEO_PATH)  # Obtient des informations sur la vidéo source\n",
        "generator = get_video_frames_generator(SOURCE_VIDEO_PATH)  # Génère des images de la vidéo source\n",
        "box_annotator = BoxAnnotator(color=ColorPalette(), thickness=2, text_thickness=2, text_scale=1)  # Crée une instance de BoxAnnotator pour annoter les boîtes\n",
        "\n",
        "# Ouvre le fichier vidéo cible pour écriture\n",
        "with VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "    # Parcours des images de la vidéo en boucle\n",
        "    for frame in tqdm(generator, total=video_info.total_frames):\n",
        "        # Prédiction du modèle sur une image individuelle et conversion en détections de supervision\n",
        "        results = model(frame, classes=[0])\n",
        "        detections = Detections(\n",
        "            xyxy=results[0].boxes.xyxy.cpu().numpy(),\n",
        "            confidence=results[0].boxes.conf.cpu().numpy(),\n",
        "            class_id=results[0].boxes.cls.cpu().numpy().astype(int)\n",
        "        )\n",
        "        # Filtrage des détections avec des classes indésirables\n",
        "        massk = np.array([class_id in CLASS_ID for class_id in detections.class_id], dtype=bool)\n",
        "        detections = detections.filter(mask=massk, inplace=True)\n",
        "        # Suivi des détections\n",
        "        tracks = byte_tracker.update(\n",
        "            output_results=detections2boxes(detections=detections),\n",
        "            img_info=frame.shape,\n",
        "            img_size=frame.shape\n",
        "        )\n",
        "        tracker_id = match_detections_with_tracks(detections=detections, tracks=tracks)\n",
        "        detections.tracker_id = np.array(tracker_id)\n",
        "        # Filtrage des détections sans suivi\n",
        "        maskk = np.array([tracker_id is not None for tracker_id in detections.tracker_id], dtype=bool)\n",
        "        detections.filter(mask=maskk, inplace=True)\n",
        "        # Formatage des libellés personnalisés\n",
        "        labels = [\n",
        "            f\"ID_{tracker_id}\"\n",
        "            for _, confidence, class_id, tracker_id\n",
        "            in detections\n",
        "        ]\n",
        "        # Annotation et affichage de l'image\n",
        "        frame = box_annotator.annotate(frame=frame, detections=detections, labels=labels)\n",
        "        sink.write_frame(frame)  # Écrit l'image annotée dans la vidéo cible\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2e4e98775a164a69ba414820f3634587",
            "311cb2fb5a8140179af02acb1d62dffe",
            "7493e997953f4effa1be93bc298df637",
            "283a9450fb68466f8d2bd772057a679b",
            "042b1674c6a0419e9180fd6589290099",
            "ecb1103738f54179a27df7ffd85c0337",
            "a4ca28bfd9e84765a4a5e05ebfbd25e8",
            "c2dd924ad7494871ae4230f0e883bcb0",
            "15a178e0dfa8479c97496a03e650fbb5",
            "d5c9b78d1ccf4ab1ad584c95081d83ad",
            "1f296fe3daf243a7b690d64ac44150bf"
          ]
        },
        "id": "5Veq9IiUfoqD",
        "outputId": "f7b3fb2c-82b8-4af8-d864-677fd49e31e3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/141 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e4e98775a164a69ba414820f3634587"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 (no detections), 64.0ms\n",
            "Speed: 2.2ms preprocess, 64.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 44.7ms\n",
            "Speed: 2.1ms preprocess, 44.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 39.3ms\n",
            "Speed: 2.0ms preprocess, 39.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 39.5ms\n",
            "Speed: 2.1ms preprocess, 39.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 35.9ms\n",
            "Speed: 2.0ms preprocess, 35.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 35.5ms\n",
            "Speed: 1.9ms preprocess, 35.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.0ms\n",
            "Speed: 1.8ms preprocess, 31.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 30.7ms\n",
            "Speed: 2.1ms preprocess, 30.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 30.5ms\n",
            "Speed: 2.2ms preprocess, 30.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 30.7ms\n",
            "Speed: 1.8ms preprocess, 30.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.1ms\n",
            "Speed: 3.8ms preprocess, 31.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.8ms\n",
            "Speed: 2.0ms preprocess, 31.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.3ms\n",
            "Speed: 1.8ms preprocess, 31.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.0ms\n",
            "Speed: 2.0ms preprocess, 31.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 30.8ms\n",
            "Speed: 2.3ms preprocess, 30.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 30.6ms\n",
            "Speed: 1.8ms preprocess, 30.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 29.6ms\n",
            "Speed: 4.3ms preprocess, 29.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 30.4ms\n",
            "Speed: 2.0ms preprocess, 30.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.1ms\n",
            "Speed: 3.9ms preprocess, 31.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.2ms\n",
            "Speed: 2.4ms preprocess, 31.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.8ms\n",
            "Speed: 1.8ms preprocess, 31.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 30.1ms\n",
            "Speed: 2.1ms preprocess, 30.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 33.9ms\n",
            "Speed: 2.4ms preprocess, 33.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.6ms\n",
            "Speed: 1.8ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.2ms\n",
            "Speed: 1.9ms preprocess, 32.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 31.1ms\n",
            "Speed: 2.4ms preprocess, 31.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.0ms\n",
            "Speed: 1.9ms preprocess, 32.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 30.7ms\n",
            "Speed: 4.5ms preprocess, 30.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.5ms\n",
            "Speed: 4.3ms preprocess, 31.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.3ms\n",
            "Speed: 2.2ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.8ms\n",
            "Speed: 2.2ms preprocess, 31.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 31.4ms\n",
            "Speed: 1.8ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 32.0ms\n",
            "Speed: 1.8ms preprocess, 32.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.5ms\n",
            "Speed: 1.9ms preprocess, 31.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.5ms\n",
            "Speed: 1.7ms preprocess, 31.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 30.6ms\n",
            "Speed: 2.1ms preprocess, 30.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 32.7ms\n",
            "Speed: 1.7ms preprocess, 32.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.6ms\n",
            "Speed: 2.2ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 33.3ms\n",
            "Speed: 1.8ms preprocess, 33.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 30.7ms\n",
            "Speed: 2.3ms preprocess, 30.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.1ms\n",
            "Speed: 1.9ms preprocess, 32.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 30.0ms\n",
            "Speed: 1.9ms preprocess, 30.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.1ms\n",
            "Speed: 2.3ms preprocess, 31.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.5ms\n",
            "Speed: 2.3ms preprocess, 31.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.9ms\n",
            "Speed: 1.9ms preprocess, 31.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.6ms\n",
            "Speed: 1.7ms preprocess, 31.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 32.8ms\n",
            "Speed: 1.8ms preprocess, 32.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.4ms\n",
            "Speed: 1.6ms preprocess, 31.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 32.2ms\n",
            "Speed: 1.8ms preprocess, 32.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 30.2ms\n",
            "Speed: 2.4ms preprocess, 30.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 32.9ms\n",
            "Speed: 1.8ms preprocess, 32.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 30.3ms\n",
            "Speed: 1.9ms preprocess, 30.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.2ms\n",
            "Speed: 1.9ms preprocess, 32.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.7ms\n",
            "Speed: 2.0ms preprocess, 32.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 33.2ms\n",
            "Speed: 2.1ms preprocess, 33.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.8ms\n",
            "Speed: 2.2ms preprocess, 32.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.8ms\n",
            "Speed: 2.0ms preprocess, 31.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 32.6ms\n",
            "Speed: 2.8ms preprocess, 32.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 33.1ms\n",
            "Speed: 1.7ms preprocess, 33.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 29.6ms\n",
            "Speed: 2.0ms preprocess, 29.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 33.1ms\n",
            "Speed: 2.1ms preprocess, 33.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.4ms\n",
            "Speed: 2.1ms preprocess, 32.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.9ms\n",
            "Speed: 1.8ms preprocess, 32.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 30.9ms\n",
            "Speed: 2.3ms preprocess, 30.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 33.3ms\n",
            "Speed: 2.0ms preprocess, 33.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 33.0ms\n",
            "Speed: 2.4ms preprocess, 33.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.6ms\n",
            "Speed: 2.1ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 33.6ms\n",
            "Speed: 1.8ms preprocess, 33.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.3ms\n",
            "Speed: 3.0ms preprocess, 32.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.0ms\n",
            "Speed: 2.0ms preprocess, 32.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.8ms\n",
            "Speed: 5.9ms preprocess, 31.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.2ms\n",
            "Speed: 2.0ms preprocess, 31.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 33.0ms\n",
            "Speed: 2.3ms preprocess, 33.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 32.2ms\n",
            "Speed: 1.7ms preprocess, 32.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 33.0ms\n",
            "Speed: 1.9ms preprocess, 33.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.4ms\n",
            "Speed: 2.3ms preprocess, 32.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 31.5ms\n",
            "Speed: 3.4ms preprocess, 31.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.2ms\n",
            "Speed: 6.7ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 32.7ms\n",
            "Speed: 1.9ms preprocess, 32.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 30.8ms\n",
            "Speed: 4.5ms preprocess, 30.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.5ms\n",
            "Speed: 1.9ms preprocess, 31.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.1ms\n",
            "Speed: 2.1ms preprocess, 31.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.0ms\n",
            "Speed: 4.6ms preprocess, 32.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.7ms\n",
            "Speed: 2.1ms preprocess, 31.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 32.5ms\n",
            "Speed: 2.2ms preprocess, 32.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 31.9ms\n",
            "Speed: 2.6ms preprocess, 31.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.5ms\n",
            "Speed: 1.9ms preprocess, 32.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 30.8ms\n",
            "Speed: 2.8ms preprocess, 30.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 32.6ms\n",
            "Speed: 1.9ms preprocess, 32.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 30.5ms\n",
            "Speed: 1.9ms preprocess, 30.5ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.2ms\n",
            "Speed: 1.9ms preprocess, 31.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.1ms\n",
            "Speed: 1.7ms preprocess, 31.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.1ms\n",
            "Speed: 2.0ms preprocess, 31.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.3ms\n",
            "Speed: 2.0ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.5ms\n",
            "Speed: 1.8ms preprocess, 32.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.3ms\n",
            "Speed: 1.8ms preprocess, 32.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.9ms\n",
            "Speed: 2.1ms preprocess, 32.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.4ms\n",
            "Speed: 2.0ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 33.2ms\n",
            "Speed: 2.5ms preprocess, 33.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 32.8ms\n",
            "Speed: 2.4ms preprocess, 32.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.7ms\n",
            "Speed: 2.0ms preprocess, 32.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.4ms\n",
            "Speed: 2.0ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.9ms\n",
            "Speed: 2.1ms preprocess, 31.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.5ms\n",
            "Speed: 1.9ms preprocess, 31.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.9ms\n",
            "Speed: 1.5ms preprocess, 31.9ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.6ms\n",
            "Speed: 1.8ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 32.2ms\n",
            "Speed: 1.8ms preprocess, 32.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.7ms\n",
            "Speed: 8.7ms preprocess, 31.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 32.5ms\n",
            "Speed: 1.9ms preprocess, 32.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.2ms\n",
            "Speed: 1.8ms preprocess, 31.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 32.9ms\n",
            "Speed: 1.8ms preprocess, 32.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.8ms\n",
            "Speed: 1.8ms preprocess, 31.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.6ms\n",
            "Speed: 2.0ms preprocess, 32.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.4ms\n",
            "Speed: 2.1ms preprocess, 32.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.5ms\n",
            "Speed: 2.1ms preprocess, 32.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.7ms\n",
            "Speed: 4.9ms preprocess, 31.7ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.2ms\n",
            "Speed: 2.1ms preprocess, 32.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.3ms\n",
            "Speed: 2.6ms preprocess, 31.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 30.8ms\n",
            "Speed: 6.0ms preprocess, 30.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.1ms\n",
            "Speed: 2.6ms preprocess, 31.1ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.3ms\n",
            "Speed: 2.1ms preprocess, 31.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 (no detections), 31.4ms\n",
            "Speed: 1.9ms preprocess, 31.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.4ms\n",
            "Speed: 2.0ms preprocess, 31.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.7ms\n",
            "Speed: 2.9ms preprocess, 31.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.0ms\n",
            "Speed: 1.7ms preprocess, 32.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.3ms\n",
            "Speed: 1.9ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.0ms\n",
            "Speed: 3.4ms preprocess, 31.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 30.8ms\n",
            "Speed: 2.1ms preprocess, 30.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.8ms\n",
            "Speed: 2.1ms preprocess, 32.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 33.0ms\n",
            "Speed: 2.7ms preprocess, 33.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.5ms\n",
            "Speed: 1.7ms preprocess, 32.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.4ms\n",
            "Speed: 5.9ms preprocess, 31.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.0ms\n",
            "Speed: 2.7ms preprocess, 32.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.0ms\n",
            "Speed: 2.1ms preprocess, 32.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 33.1ms\n",
            "Speed: 2.1ms preprocess, 33.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.5ms\n",
            "Speed: 2.2ms preprocess, 32.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.7ms\n",
            "Speed: 1.9ms preprocess, 32.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.8ms\n",
            "Speed: 1.8ms preprocess, 31.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 32.3ms\n",
            "Speed: 2.0ms preprocess, 32.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 31.4ms\n",
            "Speed: 1.9ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 31.8ms\n",
            "Speed: 2.6ms preprocess, 31.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pdMUzc9D6OCJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}